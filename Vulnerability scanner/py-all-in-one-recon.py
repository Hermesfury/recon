#!/usr/bin/env python3
"""
Web Information Gathering Tool
Author: Hermesfurie
Purpose: Ethical OSINT & Recon (Authorized Use Only) do not use on websites if you dont have permission 
"""

import requests
import socket
import re
import sys
import json
import whois
import dns.resolver
import ssl
import subprocess
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup, Comment
from colorama import Fore, Style, init
from datetime import datetime
import os

# Initialize colorama for colored output on Windows/Linux
init(autoreset=True)

# ===========================================
# Global Logger Setup
# ===========================================

class DualLogger:
    """Log to both console and file simultaneously."""
    def __init__(self, filename):
        self.filename = filename
        self.file = open(filename, 'w', encoding='utf-8')
    
    def write_log(self, message, level="INFO"):
        """Write to both console and file."""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] [{level}] {message}"
        
        # Write to file
        self.file.write(log_entry + "\n")
        self.file.flush()
        
        # Write to console
        if level == "SUCCESS":
            print(f"{Fore.GREEN}[+]{Style.RESET_ALL} {message}")
        elif level == "WARNING":
            print(f"{Fore.YELLOW}[!]{Style.RESET_ALL} {message}")
        elif level == "ERROR":
            print(f"{Fore.RED}[-]{Style.RESET_ALL} {message}")
        else:
            print(f"{Fore.BLUE}[*]{Style.RESET_ALL} {message}")
    
    def close(self):
        """Close the log file."""
        self.file.close()

# Create logs directory
if not os.path.exists("recon_logs"):
    os.makedirs("recon_logs")

# Initialize logger with timestamp
log_filename = f"recon_logs/recon_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logger = DualLogger(log_filename)

def log_info(msg):
    logger.write_log(msg, "INFO")

def log_success(msg):
    logger.write_log(msg, "SUCCESS")

def log_warning(msg):
    logger.write_log(msg, "WARNING")

def log_error(msg):
    logger.write_log(msg, "ERROR")

# ===========================================
# Utility Functions
# ===========================================

def banner():
    banner_text = r"""
    __          __  _                               
    \ \        / / | |                            
     \ \  /\  / /__| | ___ ___  _ __ ___   ___  
      \ \/  \/ / _ \ |/ __/ _ \| '_ ` _ \ / _ \ 
       \  /\  /  __/ | (_| (_) | | | | | |  __/  
        \/  \/ \___|_|\___\___\|_| |_| |_|\___| 
          
          |                     |
          |     HERMESFURIE TOOL     |
          |     ENHANCED RECON    |
        Web Information Gathering Tool do not use on websites if you dont have permission 
    """
    print(Fore.CYAN + Style.BRIGHT + banner_text + Style.RESET_ALL)
    logger.write_log("="*60 + " RECONNAISSANCE STARTED " + "="*60, "INFO")
    print(Fore.YELLOW + "[!] For authorized use only. Respect robots.txt and privacy." + Style.RESET_ALL)
    print("-" * 60 + "\n")

def request_url(url, timeout=10):
    """Make HTTP request with proper error handling."""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    try:
        response = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True, verify=False)
        return response
    except requests.exceptions.SSLError:
        log_warning("SSL Error: Trying HTTP instead of HTTPS...")
        try:
            response = requests.get(f"http://{url.split('://')[1]}", headers=headers, timeout=timeout, verify=False)
            return response
        except:
            log_error("HTTP also failed.")
            return None
    except requests.exceptions.RequestException as e:
        log_error(f"Request failed: {e}")
        return None

def is_valid_domain(domain):
    """Basic domain validation."""
    pattern = r"^(?!-)[A-Za-z0-9-]+([\-\.]{1}[a-z0-9]+)*\.[A-Za-z]{2,}$"
    return re.match(pattern, domain) is not None

def get_domain_and_ip(target):
    """Resolve domain to IP."""
    domain = target.lower().strip().replace("http://", "").replace("https://", "").split("/")[0]
    if not is_valid_domain(domain):
        log_error("Invalid domain format.")
        return None, None
    try:
        ip = socket.gethostbyname(domain)
        log_success(f"Domain: {domain}")
        log_success(f"Primary IP: {ip}")
        return domain, ip
    except socket.gaierror:
        log_error("Could not resolve domain to IP.")
        return domain, None

# ===========================================
# Module 0: WHOIS & DNS Info
# ===========================================

def get_whois_info(domain):
    """Fetch WHOIS information."""
    log_info("Fetching WHOIS information...")
    try:
        w = whois.whois(domain)
        if w:
            log_success(f"Registrar: {w.registrar}")
            if w.creation_date:
                log_success(f"Created: {w.creation_date}")
            if w.expiration_date:
                log_success(f"Expires: {w.expiration_date}")
            if w.name_servers:
                log_success(f"Name Servers: {', '.join(w.name_servers)}")
            if w.org:
                log_success(f"Organization: {w.org}")
    except Exception as e:
        log_warning(f"WHOIS lookup failed: {e}")

def get_dns_records(domain):
    """Fetch DNS A, MX, NS, CNAME, SOA records."""
    log_info("Fetching DNS records...")
    try:
        # A Records
        a_records = dns.resolver.resolve(domain, 'A')
        ips = [str(r) for r in a_records]
        log_success(f"A Records: {', '.join(ips)}")
    except:
        pass

    try:
        # AAAA Records (IPv6)
        aaaa_records = dns.resolver.resolve(domain, 'AAAA')
        ipv6 = [str(r) for r in aaaa_records]
        log_success(f"AAAA Records: {', '.join(ipv6)}")
    except:
        pass

    try:
        # MX Records
        mx_records = dns.resolver.resolve(domain, 'MX')
        mx = [str(r.exchange) for r in mx_records]
        log_success(f"MX Records: {', '.join(mx)}")
    except:
        pass

    try:
        # NS Records
        ns_records = dns.resolver.resolve(domain, 'NS')
        ns = [str(r) for r in ns_records]
        log_success(f"NS Records: {', '.join(ns)}")
    except:
        pass

    try:
        # CNAME Records
        cname_records = dns.resolver.resolve(domain, 'CNAME')
        cname = [str(r) for r in cname_records]
        log_success(f"CNAME Records: {', '.join(cname)}")
    except:
        pass

    try:
        # SOA Records
        soa_records = dns.resolver.resolve(domain, 'SOA')
        soa = [str(r) for r in soa_records]
        log_success(f"SOA Records: {', '.join(soa)}")
    except:
        pass

    try:
        # TXT Records
        txt_records = dns.resolver.resolve(domain, 'TXT')
        txt = [str(r) for r in txt_records]
        if txt:
            log_info(f"TXT Records Found: {len(txt)}")
    except:
        pass

# ===========================================
# Module 0.5: Subdomain Enumeration
# ===========================================

def enumerate_subdomains(domain):
    """Enumerate subdomains using DNS brute-force."""
    log_info("Enumerating subdomains...")
    
    common_subdomains = [
        "www", "mail", "ftp", "localhost", "webmail", "smtp", "pop", "ns1",
        "webdisk", "ns2", "cpanel", "whois", "autodiscover", "autoconfig",
        "m", "api", "admin", "test", "portal", "staging", "dev", "development",
        "cdn", "static", "app", "blog", "shop", "store", "news", "support",
        "help", "docs", "download", "files", "images", "assets", "backup",
        "vpn", "git", "github", "jenkins", "docker", "kubernetes", "monitoring",
        "grafana", "prometheus", "elastic", "kibana", "splunk", "siem",
        "internal", "secret", "private", "hidden", "old", "archive", "legacy"
    ]
    
    found_subdomains = []
    
    for subdomain in common_subdomains:
        try:
            full_domain = f"{subdomain}.{domain}"
            ip = socket.gethostbyname(full_domain)
            found_subdomains.append(full_domain)
            log_success(f"Subdomain Found: {full_domain} ({ip})")
        except socket.gaierror:
            continue
        except:
            continue
    
    if not found_subdomains:
        log_info("No subdomains found via brute-force.")
    
    return found_subdomains

# ===========================================
# Module 1: SSL/TLS Certificate Analysis
# ===========================================

def analyze_ssl_certificate(domain):
    """Analyze SSL/TLS certificate."""
    log_info("Analyzing SSL/TLS Certificate...")
    try:
        context = ssl.create_default_context()
        with socket.create_connection((domain, 443), timeout=10) as sock:
            with context.wrap_socket(sock, server_hostname=domain) as ssock:
                cert = ssock.getpeercert()
                
                log_success(f"Subject: {dict(x[0] for x in cert['subject'])}")
                log_success(f"Issuer: {dict(x[0] for x in cert['issuer'])}")
                log_success(f"Not Valid Before: {cert['notBefore']}")
                log_success(f"Not Valid After: {cert['notAfter']}")
                
                # Check certificate expiration
                not_after = datetime.strptime(cert['notAfter'], '%b %d %H:%M:%S %Y %Z')
                days_left = (not_after - datetime.now()).days
                
                if days_left < 30:
                    log_warning(f"Certificate expires in {days_left} days!")
                else:
                    log_success(f"Certificate valid for {days_left} more days")
                
                # SANs
                if 'subjectAltName' in cert:
                    san_list = [item[1] for item in cert['subjectAltName']]
                    log_success(f"Alternative Names: {', '.join(san_list)}")
    except Exception as e:
        log_warning(f"SSL Certificate analysis failed: {e}")

# ===========================================
# Module 1.5: Server & Technology Detection
# ===========================================

def detect_technologies(response, url):
    """Detect server, CMS, frameworks, etc."""
    log_info("Detecting technologies...")

    # Server header
    server = response.headers.get("Server", "Unknown")
    log_success(f"Web Server: {server}")

    # X-Powered-By
    powered = response.headers.get("X-Powered-By")
    if powered:
        log_success(f"Powered By: {powered}")

    # Check other tech headers
    if response.headers.get("X-AspNet-Version"):
        log_success(f"ASP.NET Version: {response.headers.get('X-AspNet-Version')}")
    if response.headers.get("X-Drupal-Cache"):
        log_success("Drupal Detected")

    html = response.text.lower()

    # CMS & Framework Signatures (EXPANDED)
    signatures = {
        "WordPress": r"/wp-content/|WordPress|wp-includes|wp-json|wp-admin",
        "Joomla": r"content=\"Joomla|Joomla!|/components/com_",
        "Drupal": r"Drupal|drupal\.org|sites/default/files|/modules/",
        "Magento": r"Magento|magento|/media/catalog/|/skin/",
        "Shopify": r"Shopify|CDN\.shopify|myshopify",
        "React": r"react-dom|/_next/static|__REACT|react\.js",
        "Vue.js": r"vue\.js|Vue\.config|__VUE__|vue/dist",
        "Angular": r"angular\.min\.js|ng-app|ng-controller",
        "Next.js": r"/_next/|next\.js",
        "Nginx": r"nginx",
        "Apache": r"Apache",
        "Cloudflare": r"cloudflare",
        "PHP": r"PHPSESSID|\.php|php/",
        "ASP.NET": r"ASP\.NET|\.aspx|__VIEWSTATE|System.Web",
        "Node.js": r"express|node\.js|nodejs",
        "Python": r"python|flask|django|wsgi",
        "Java": r"java|jsp|tomcat|spring",
        "Bootstrap": r"bootstrap\.css|bootstrap\.min|twitter/bootstrap",
        "jQuery": r"jquery\.js|jquery\.min|jquery/",
        "Chart.js": r"chart\.js",
        "D3.js": r"d3\.js|d3\.min",
        "Webpack": r"webpack",
        "TypeScript": r"typescript|\.ts"
    }

    detected = set()
    for tech, pattern in signatures.items():
        if re.search(pattern, html, re.I):
            detected.add(tech)

    # Check via headers too
    header_str = str(response.headers).lower()
    if "nginx" in server.lower():
        detected.add("Nginx")
    if "apache" in server.lower():
        detected.add("Apache")
    if "cloudflare" in header_str:
        detected.add("Cloudflare")

    if detected:
        log_success(f"Technologies: {', '.join(sorted(detected))}")
    else:
        log_info("No clear technologies detected.")

# ===========================================
# Module 2: Extract Page Info & Endpoints
# ===========================================

def extract_page_info(response):
    """Extract title, emails, links, comments, endpoints."""
    soup = BeautifulSoup(response.text, "html.parser")

    # 1. Page Title
    title = soup.find("title")
    if title:
        title_text = title.get_text().strip()
        log_success(f"Page Title: {title_text}")

    # 2. Meta Description
    desc = soup.find("meta", attrs={"name": "description"})
    if desc:
        content = desc.get("content", "")
        log_info(f"Meta Description: {content[:100]}")

    # 3. All Links (improved)
    links = soup.find_all("a", href=True)
    internal, external = set(), set()
    api_endpoints = set()
    
    base_domain = re.escape(response.url.split("://")[1].split("/")[0])

    for link in links:
        href = link["href"].strip()
        if href and not href.startswith("javascript:") and not href.startswith("#"):
            full_url = urljoin(response.url, href)
            
            # Detect API endpoints
            if "/api/" in full_url.lower() or "/v1/" in full_url.lower() or "/v2/" in full_url.lower():
                api_endpoints.add(full_url)
            
            if re.search(base_domain, full_url):
                internal.add(full_url)
            else:
                external.add(full_url)

    if internal:
        log_success(f"Internal Links: {len(internal)} found")
    if external:
        log_info(f"External Links: {len(external)} found")
    if api_endpoints:
        log_success(f"API Endpoints Found: {len(api_endpoints)}")
        for endpoint in list(api_endpoints)[:5]:
            log_info(f"  - {endpoint}")

    # 4. Email addresses (improved regex)
    emails = set(re.findall(r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}", response.text))
    if emails:
        log_success(f"Emails Found: {', '.join(sorted(emails))}")

    # 5. Phone numbers
    phones = set(re.findall(r"(\+?1?)[-.\s]?\(?[0-9]{3}\)?[-.\s]?[0-9]{3}[-.\s]?[0-9]{4}", response.text))
    if phones:
        log_info(f"Phone Numbers: {', '.join(sorted(phones))}")

    # 6. HTML Comments
    comments = soup.find_all(string=lambda text: isinstance(text, Comment))
    sensitive_keywords = ["password", "key", "secret", "token", "debug", "backup", "config", "api", "access", "admin", "user", "database", "db", "query"]
    sensitive_found = 0
    for comment in comments:
        c = comment.strip()
        if len(c) > 0:
            if any(kw in c.lower() for kw in sensitive_keywords):
                log_warning(f"Sensitive Comment: {c[:100]}")
                sensitive_found += 1

    if sensitive_found == 0 and len(comments) > 0:
        log_info(f"Total HTML Comments: {len(comments)}")

    # 7. Forms
    forms = soup.find_all("form")
    if forms:
        log_success(f"Forms Detected: {len(forms)}")
        for i, form in enumerate(forms):
            action = form.get("action", "unknown")
            method = form.get("method", "GET").upper()
            log_info(f"  Form {i+1}: action={action}, method={method}")

    # 8. Scripts (detect external scripts)
    scripts = soup.find_all("script", src=True)
    if scripts:
        log_info(f"External Scripts: {len(scripts)}")
        for script in scripts[:3]:
            log_info(f"  - {script.get('src')}")

    # 9. Iframes
    iframes = soup.find_all("iframe")
    if iframes:
        log_warning(f"Iframes Found: {len(iframes)}")
        for iframe in iframes[:3]:
            log_warning(f"  - {iframe.get('src')}")

# ===========================================
# Module 3: Port Scanner (Enhanced)
# ===========================================

COMMON_PORTS = {
    80: "HTTP", 443: "HTTPS", 21: "FTP", 22: "SSH", 25: "SMTP",
    110: "POP3", 143: "IMAP", 3306: "MySQL", 5432: "PostgreSQL",
    6379: "Redis", 27017: "MongoDB", 3389: "RDP", 8080: "HTTP-Alt",
    8443: "HTTPS-Alt", 9000: "SonarQube", 8888: "Jupyter", 5000: "Flask",
    3000: "Node.js", 9200: "Elasticsearch", 5601: "Kibana", 8081: "Tomcat"
}

def scan_common_ports(ip):
    """Scan common ports on the target IP."""
    log_info(f"Scanning common ports on {ip}...")
    open_ports = []

    for port, service in COMMON_PORTS.items():
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(1)
            result = sock.connect_ex((ip, port))
            if result == 0:
                open_ports.append(f"{port} ({service})")
                log_success(f"Port Open: {port} → {service}")
            sock.close()
        except:
            continue

    if not open_ports:
        log_info("No common ports found open.")
    return open_ports

# ===========================================
# Module 4: HTTP Security Headers
# ===========================================

SECURITY_HEADERS = {
    "Strict-Transport-Security": "HSTS",
    "Content-Security-Policy": "CSP",
    "X-Content-Type-Options": "nosniff",
    "X-Frame-Options": "Clickjacking",
    "X-XSS-Protection": "XSS Filter",
    "Referrer-Policy": "Referrer",
    "Permissions-Policy": "Permissions",
    "Access-Control-Allow-Origin": "CORS"
}

def check_security_headers(response):
    """Check for missing security headers."""
    log_info("Checking security headers...")
    headers = response.headers
    missing = []
    present = []

    for header, description in SECURITY_HEADERS.items():
        if header in headers:
            log_success(f"✓ {description}: {headers.get(header)[:50]}")
            present.append(header)
        else:
            missing.append(f"{description} ({header})")

    if missing:
        log_warning(f"Missing Headers ({len(missing)}): {', '.join([m.split('(')[0].strip() for m in missing])}")
    else:
        log_success("All security headers present!")

# ===========================================
# Module 5: Vulnerability Check
# ===========================================

def check_vulnerabilities(domain, response):
    """Check for common vulnerabilities."""
    log_info("Checking for common vulnerabilities...")
    
    vulns = []
    
    # Check for outdated software
    server = response.headers.get("Server", "").lower()
    if "apache/2.2" in server or "apache/2.0" in server:
        vulns.append("Outdated Apache version detected")
    
    if "php/5" in server:
        vulns.append("PHP 5.x detected (outdated)")
    
    # Check for information disclosure
    headers = response.headers
    if "X-Powered-By" in headers:
        vulns.append(f"Server disclosure: {headers['X-Powered-By']}")
    
    if "Server" in headers and "Unknown" not in headers["Server"]:
        vulns.append(f"Server version disclosed: {headers['Server']}")
    
    # Check for missing security headers
    if "X-Frame-Options" not in headers:
        vulns.append("X-Frame-Options not set (Clickjacking vulnerability)")
    
    if "X-Content-Type-Options" not in headers:
        vulns.append("X-Content-Type-Options not set (MIME sniffing)")
    
    if "Content-Security-Policy" not in headers:
        vulns.append("Content-Security-Policy not set (XSS vulnerability)")
    
    # Check for HTTPS redirect
    try:
        http_response = requests.get(f"http://{domain}", timeout=5, allow_redirects=False, verify=False)
        if http_response.status_code not in [301, 302, 303, 307, 308]:
            vulns.append("No automatic HTTPS redirect (HTTP allowed)")
    except:
        pass
    
    if vulns:
        log_warning(f"Potential Vulnerabilities ({len(vulns)}):")
        for vuln in vulns:
            log_warning(f"  - {vuln}")
    else:
        log_success("No obvious vulnerabilities detected")

# ===========================================
# Main Execution
# ===========================================

def main():
    banner()

    target = input(Fore.CYAN + "Enter website URL or domain: " + Style.RESET_ALL).strip()

    if not target.startswith("http://") and not target.startswith("https://"):
        target = "https://" + target

    # Extract domain and resolve IP
    domain, ip = get_domain_and_ip(target)
    if not domain:
        sys.exit(1)

    log_info(f"Fetching: {target}")
    response = request_url(target)
    if not response:
        log_error("Failed to fetch website.")
        sys.exit(1)

    log_success(f"Status Code: {response.status_code}")
    print("\n" + "="*60)

    # Run all modules
    print(f"\n{Fore.CYAN}[WHOIS & DNS]{Style.RESET_ALL}")
    print("="*60)
    log_info("========== WHOIS & DNS ==========")
    get_whois_info(domain)
    get_dns_records(domain)
    
    print(f"\n{Fore.CYAN}[SUBDOMAIN ENUMERATION]{Style.RESET_ALL}")
    print("="*60)
    log_info("========== SUBDOMAIN ENUMERATION ==========")
    enumerate_subdomains(domain)
    
    print(f"\n{Fore.CYAN}[SSL/TLS ANALYSIS]{Style.RESET_ALL}")
    print("="*60)
    log_info("========== SSL/TLS ANALYSIS ==========")
    analyze_ssl_certificate(domain)
    
    print(f"\n{Fore.CYAN}[TECHNOLOGY DETECTION]{Style.RESET_ALL}")
    print("="*60)
    log_info("========== TECHNOLOGY DETECTION ==========")
    detect_technologies(response, target)
    
    print(f"\n{Fore.CYAN}[PAGE & ENDPOINT ANALYSIS]{Style.RESET_ALL}")
    print("="*60)
    log_info("========== PAGE & ENDPOINT ANALYSIS ==========")
    extract_page_info(response)
    
    print(f"\n{Fore.CYAN}[SECURITY HEADERS]{Style.RESET_ALL}")
    print("="*60)
    log_info("========== SECURITY HEADERS ==========")
    check_security_headers(response)
    
    print(f"\n{Fore.CYAN}[VULNERABILITY CHECK]{Style.RESET_ALL}")
    print("="*60)
    log_info("========== VULNERABILITY CHECK ==========")
    check_vulnerabilities(domain, response)
    
    print(f"\n{Fore.CYAN}[PORT SCANNING]{Style.RESET_ALL}")
    print("="*60)
    log_info("========== PORT SCANNING ==========")
    if ip:
        scan_common_ports(ip)

    print("\n" + "="*60)
    log_success("Reconnaissance completed!")
    print("="*60 + "\n")
    
    # Log file summary
    log_info(f"Full log saved to: {log_filename}")
    print(f"{Fore.GREEN}[+] Results saved to: {log_filename}{Style.RESET_ALL}\n")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        log_warning("\nInterrupted by user.")
    except Exception as e:
        log_error(f"Error: {e}")
    finally:
        logger.close()
        print(f"{Fore.CYAN}[*] Log file closed.{Style.RESET_ALL}")